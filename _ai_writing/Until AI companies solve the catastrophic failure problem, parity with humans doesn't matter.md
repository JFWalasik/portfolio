---
title: Until AI companies solve the catastrophic failure problem, parity with humans doesn’t matter
description: A discussion of "parity" with humans vs. catastrophic AI errors 
date: 2025-11-01
---

**Until AI companies solve the "catastrophic" failure problem, "parity"
with humans doesn’t matter**

OpenAI just released a
[widely](https://futurism.com/future-society/openai-work-tasks-chatgpt-can-already-replace)-[reported](https://fortune.com/2025/09/30/ai-models-are-already-as-good-as-experts-at-half-of-tasks-a-new-openai-benchmark-gdpval-suggests/)
[paper](https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf)
announcing that GPT-5 has approached “human parity” on many professional
tasks. But buried on pages 15-16 of the report, they mention that nearly
3% of GPT-5s failures are “catastrophic.” OpenAI defines catastrophic
as: “harmful or dangerously wrong (e.g., insulting a customer, giving
the wrong diagnosis, recommending fraud, or suggesting actions that will
cause physical harm).”

Catastrophic failure is not the [point of the
paper](https://openai.com/index/gdpval/), though, which is about using
their new tool, GDPval, to “help us track how well our models and others
perform on economically valuable, real-world tasks.”

After identifying “real world” tasks from 44 occupations across 9
industries, OpenAI used GDPval to evaluate AI models’ performance
compared to humans. Through an evaluation process involving both AI and
human experts, they found that GPT-5 performed equal to or better than
industry professionals on 38.8% of these tasks. Notably, GPT-5
outperformed Gemini 2.5 Pro and Grok 4, both with about a 25% success
rate, but Claude Opus 4.1 did even better, equaling or outperforming the
human professional 47.6% of the time – near parity, in their estimation.
<br>
**AI failures remain**
<br>
Yet, the failures remain notable and extreme. AI
[hallucinations](https://www.theverge.com/2023/2/14/23599007/microsoft-bing-ai-mistakes-demo)
[and](https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt)
[errors](https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo)
have embarrassed and distressed many, while OpenAI has been
[sued](https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147)
for alleged “suicide coaching.” Bing AI famously [professed its
love](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
for a New York Times writer and suggested he leave his wife for the
chatbot.

Some of these might be considered “catastrophic” AI errors, and
according to the paper from the OpenAI team, these catastrophic outcomes
still occur in 2.7% of GPT-5’s failures. Since they report an overall
success rate of 38.8% (and therefore a failure rate of 61.2%), this
catastrophic failure rate would be **1.7%** (61.2% X 2.7%) **of all the
tasks performed**.

Again, this is not the point of the paper, which introduces the new tool
for evaluating AI model performance, and argues that AI models “are
beginning to approach parity with industry experts” on a subset of
tasks.
<br>
**What level of catastrophic failure is acceptable?**
<br>
But what industry or profession allows “experts” to catastrophically
fail nearly 2% of the time? Health care professionals? One or two
catastrophic failures out of 100 patients would likely end a career.
Airline pilots or maintenance workers? No. Lawyers (see the
[article](https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt)
about lawyers presenting AI-hallucinated case citations in court)?
Maybe.

What level of catastrophic failure is acceptable? Pretty close to zero
in most professions, probably. Even at the milder end – customer service
– imagine a retail worker who insults or yells at a customer only once a
day in which they have 50 customer interactions. They would still be
known as “the guy who yells at people,” and soon, “that guy who used to
work here who yelled at people.”
<br>
**A Better Way – Move Fast and Break Nothing**
<br>
Waymo’s track record indicates that this problem is not unsolvable. As
Saahil Desai reports in *The Atlantic*, Waymo’s “[Move Fast and Break
Nothing](https://www.theatlantic.com/technology/2025/10/is-waymo-safe/684432/)”
strategy may be a viable long-term solution for deploying AI where
catastrophic failures could destroy the company. (This is not hyperbole:
GM’s Cruise robotaxi service [never restarted
operations](https://www.reuters.com/business/autos-transportation/general-motors-acquires-full-ownership-cruise-autonomous-business-2025-02-04/)
after an accident with a pedestrian in 2023. Though the software likely
could not have prevented the accident, [multiple sensor and software
failures](https://www.reuters.com/business/autos-transportation/how-gms-cruise-robotaxi-tech-failures-led-it-drag-pedestrian-20-feet-2024-01-26/)
contributed to running over the woman and dragging her for 20 feet.
Fortunately, the woman survived.)

Desai reports that though Waymo has existed since 2009 and has operated
its robotaxi service since 2018, [very few serious
accidents](https://www.understandingai.org/p/very-few-of-waymos-most-serious-crashes)
– and no fatal accidents – can be blamed on Waymo. Waymo’s robotaxis
aren’t perfect, Desai notes, but compared with ChatGPT, Waymo has been
much more successful at avoiding “catastrophic” failures.
<br>
**What does this mean?**
<br>
AI companies need to solve the catastrophic failure problem.

1\. Despite OpenAI’s enthusiasm and confidence in GPT-5’s “near-human”
parity, until they solve the catastrophic failure problem, AI will never
be an acceptable substitute for a real-world expert.

2\. The AI-assisted work model will likely persist for some time. As
long as “catastrophic” failures are even a remote possibility, AI will
need to be supervised by people. As Jenson Huang famously
[asserted](https://www.cnbc.com/2025/05/28/nvidia-ceo-jensen-huang-youll-lose-your-job-to-somebody-who-uses-ai.html#:~:text=Nvidia%20CEO:%20You%20won't,to%20somebody%20who%20uses%20AI'),
for now you’re much more likely to lose your job to someone who uses AI,
than to an AI model.

For OpenAI, Google, Anthropic, Meta, X, and other “frontier” AI
companies: keep working on the challenge of eliminating hallucinations
and catastrophic failures from models that were designed to predict the
best responses using weighted probabilities. For those of us who use AI
to assist our work – and thoroughly scrutinize the output and the AI’s
sources – carry on, friends.
